# ðŸ”§ Technical Architecture â€” Builder Lens (Lens 2)
## Architecture Decisions, Prompt Engineering & Data Design

> *"Be prepared to discuss your prompt engineering strategy, error handling, and data structure choices."*
> â€” Invoca Exercise Brief

---

## Why Raw Python + Anthropic API

Three options were available: n8n, Zapier, raw code.

**Zapier** was eliminated immediately. Zapier signals a low technical ceiling â€” it's a point-and-click integration tool, not an architecture. The prompt asked for someone who can *build*, not configure.

**n8n** is a legitimate choice for agentic workflow visualization and would be the right tool for a production handoff to a non-technical team. It remains a viable Phase 2 deployment layer.

**Raw Python + Anthropic API** was chosen because:
1. It's authentic â€” it mirrors how I actually build
2. It makes every architectural decision explicit and inspectable
3. The prompt engineering strategy is readable as code, not hidden behind a GUI
4. It demonstrates infrastructure-layer thinking, not just tool integration

---

## Prompt Engineering Strategy

### Architecture: Three Layers

**Layer 1 â€” System Prompt** (`prompts.py`)
Sets the model's persona as a senior CSM analyst. The persona choice is deliberate â€” a generic "extract entities" instruction produces generic output. An experienced CSM knows the difference between a customer venting and a churn signal. The system prompt calibrates that judgment.

Key constraints in the system prompt:
- JSON only â€” no prose, no markdown, no explanation outside the schema
- Extract only what is explicitly present â€” no inference beyond evidence
- Confidence score calibration defined explicitly (0.90â€“1.00 / 0.75â€“0.89 / 0.60â€“0.74 / below)
- Sentiment reflects the customer's sentiment, not the CSM's tone

**Layer 2 â€” Extraction Prompt** (`build_extraction_prompt()`)
Injects full call context before extraction: CSM name, account name, call date, optional pre-call notes.

Why context injection matters: without account context, "they mentioned Marchex" could be a casual reference. With it, the model knows this is a QBR call for an $84K account renewing in June â€” which changes the urgency classification entirely.

**Layer 3 â€” Fallback Prompt** (`build_fallback_prompt()`)
Activated when primary extraction returns invalid JSON or fails schema validation. Simplifies the output schema to 10 fields (vs. 12 in primary). Shows the model exactly what failed in its previous attempt. Two-stage fallback is better than one attempt and a hard crash.

### Prompt Versioning
Every prompt carries a version string (`PROMPT_VERSION = "1.0.0"`). In production, extraction behavior changes when prompts change. Version tracking is the audit trail.

---

## Data Structure Decisions

### Why Enums Over Strings
```python
InsightType.BUG_REPORT        # not "bug report" or "Bug Report" or "bug_report"
RoutingDestination.ENGINEERING # not "engineering" or "Engineering team"
```
Enums enforce a controlled vocabulary. When the model returns `"bug_report"`, it either matches `InsightType.BUG_REPORT` or raises a `ValueError` â€” caught and handled. Uncontrolled strings create silent routing mismatches.

### Why Separate CallMetadata from ExtractedInsight
```python
@dataclass
class CallMetadata:
    csm_name: str
    account_name: str
    ...

@dataclass
class ExtractedInsight:
    insight_type: InsightType
    ...
```
Source attribution travels with every routed alert. Engineering, Product, and Sales all see the CSM's name and the verbatim quote. The CSM is more visible through automation, not erased by it. This is the design decision that drives adoption.

### Why a Routing Rules Table
```python
ROUTING_RULES: dict[InsightType, dict] = {
    InsightType.BUG_REPORT: {
        "primary": RoutingDestination.ENGINEERING,
        "sla":     "24 hours",
    },
    ...
}
```
When Invoca adds a new team or changes an SLA, one line changes in `schema.py`. Not a chain of if/elif conditionals scattered across the codebase. Routing logic is auditable, testable, and separated from business logic.

### Why Confidence Scores Are First-Class
The confidence threshold (0.75) is a named constant in `schema.py`:
```python
CONFIDENCE_THRESHOLD = 0.75
```
Below this threshold, routing destination overrides to `RoutingDestination.HUMAN_REVIEW` regardless of insight type. The PM controls this threshold. Giving the PM that control is the adoption mechanism â€” they trust a system they can tune.

---

## Error Handling Strategy

Five named failure modes. Each has a specific handler. No generic `except Exception: pass`.

```
JSON parse failure      â†’ handle_json_parse_failure() â†’ log â†’ trigger fallback
Schema validation error â†’ handle_validation_failure() â†’ log field name â†’ trigger fallback
Confidence < threshold  â†’ routing override â†’ Human Review Queue
Empty extraction        â†’ handle_empty_extraction() â†’ clean result, no crash
API error               â†’ handle_api_error() â†’ log with transcript ID â†’ raise
```

The two-stage fallback means the pipeline degrades gracefully:
- Stage 1: Full schema extraction (12 fields)
- Stage 2: Simplified schema (10 fields, nulls allowed)
- Stage 3: Empty result with processing note

A production deployment would add: dead letter queue for failed transcripts, retry with exponential backoff on API rate limits, alerting on fallback rate exceeding a threshold.

---

## MVP Scope Decisions

**What's in the MVP:**
- Single transcript ingestion (file-based)
- Six insight types with full routing logic
- Confidence scoring with human review queue
- Structured terminal output + JSON output mode
- Mock mode for demo without API key

**What's explicitly out of MVP (and why):**
- Zendesk / Salesforce write-back: requires OAuth integration â€” adds 2 weeks, not core to demonstrating the extraction logic
- Pattern aggregation layer: requires database + time-series logic â€” Phase 2
- Slack/email webhook delivery: 30 lines of code to add, saved for Q&A
- Real-time streaming: batch processing is the right MVP choice â€” simpler, more debuggable

---

## Production Evolution Path

```
MVP (This POC)              Phase 2                    Phase 3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
File-based ingestion     â†’  API endpoint              â†’  Native Invoca stream
Single extraction agent  â†’  Multi-agent pipeline      â†’  Specialized sub-agents
Terminal output          â†’  Slack/email webhooks      â†’  Embedded in Salesforce
Manual threshold         â†’  PM-controlled dashboard   â†’  Self-tuning threshold
Batch processing         â†’  Real-time                 â†’  Live call analytics
```

---

*Lens 2: Technical Architecture | JTBD Feedback Loop POC*
*Erwin M. McDonald | Invoca Applied AI Analyst Presentation*
